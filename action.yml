name: "Ollama Runner Action"
description: "Install and run Ollama with specified models"
author: "Your Name"

inputs:
  model:
    description: "Ollama model to use (e.g., llama2, codellama, mistral)"
    required: true
    default: "smollm2:135m"
  command:
    description: "Ollama command to run (run/serve)"
    required: false
    default: "run"
  prompt:
    description: "Prompt to send to the model"
    required: false
    default: "Hello, how are you?"
  timeout:
    description: "Timeout in seconds for operations"
    required: false
    default: "300"

runs:
  using: "composite"
  steps:
    # Platform Detection
    - name: Detect Platform
      id: platform
      shell: bash
      run: |
        case "$(uname -s)" in
          Linux*)     echo "platform=linux" >> $GITHUB_OUTPUT;;
          Darwin*)    echo "platform=macos" >> $GITHUB_OUTPUT;;
          MINGW64*)   echo "platform=windows" >> $GITHUB_OUTPUT;;
          *)         echo "Unknown platform" && exit 1;;
        esac

    # Linux Installation
    - name: Install Ollama (Linux)
      if: steps.platform.outputs.platform == 'linux'
      shell: bash
      run: |
        curl -fsSL https://ollama.ai/install.sh | sh
        echo "Installed Ollama for Linux"

    - name: Install Ollama (macOS)
      if: steps.platform.outputs.platform == 'macos'
      shell: bash
      run: |
        # Check if Homebrew is installed, install if not
        if ! command -v brew &> /dev/null; then
          echo "Installing Homebrew..."
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
        fi

        # Install coreutils and Ollama using Homebrew
        echo "Installing dependencies and Ollama using Homebrew..."
        brew install coreutils ollama

        echo "Ollama installed successfully"

    - name: Install Ollama (Windows)
      if: steps.platform.outputs.platform == 'windows'
      shell: pwsh
      run: |
        Write-Host "Installing Ollama for Windows..."

        try {
          # Define installation paths
          $ollamaDir = "C:\ollama"
          $ollamaExe = "C:\ollama\ollama.exe"

          # Create installation directory
          if (-not (Test-Path $ollamaDir)) {
            New-Item -Path $ollamaDir -ItemType Directory -Force
          }

          # Get latest release info
          $apiLatestUrl = "https://api.github.com/repos/ollama/ollama/releases/latest"
          $latestRelease = Invoke-RestMethod -Uri $apiLatestUrl
          $zipUrl = $latestRelease.assets | Where-Object { $_.name -match "windows-amd64\.zip$" } | Select-Object -ExpandProperty browser_download_url

          # Download and extract
          Write-Host "Downloading Ollama from $zipUrl"
          Invoke-WebRequest -Uri $zipUrl -OutFile "$ollamaDir\ollama.zip"

          Write-Host "Extracting Ollama..."
          Expand-Archive -Path "$ollamaDir\ollama.zip" -DestinationPath $ollamaDir -Force

          # Verify installation
          if (-not (Test-Path $ollamaExe)) {
            throw "Ollama executable not found after installation"
          }

          # Add to PATH and make it available to subsequent steps
          $env:PATH = "$ollamaDir;$env:PATH"
          echo "PATH=$env:PATH" >> $env:GITHUB_ENV

          # Export the installation path for other steps
          echo "OLLAMA_DIR=$ollamaDir" >> $env:GITHUB_ENV
          echo "OLLAMA_EXE=$ollamaExe" >> $env:GITHUB_ENV

          Write-Host "Ollama installed successfully at: $ollamaExe"
          Write-Host "Current PATH: $env:PATH"

        } catch {
          Write-Host "Error during installation: $_"
          Write-Host $_.ScriptStackTrace
          exit 1
        } finally {
          # Cleanup
          if (Test-Path "$ollamaDir\ollama.zip") {
            Remove-Item "$ollamaDir\ollama.zip" -Force
          }
        }
    # Start Ollama Server (Platform Independent)
    - name: Start Ollama Server
      shell: bash
      run: |
        echo "Starting Ollama server..."

        if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
          # Windows-specific start using full path
          OLLAMA_EXE="C:/ollama/ollama.exe"
          echo "Starting Ollama from: $OLLAMA_EXE"

          powershell -Command "& {
            try {
              if (-not (Test-Path '$OLLAMA_EXE')) {
                throw 'Ollama executable not found at: $OLLAMA_EXE'
              }

              Start-Process '$OLLAMA_EXE' -ArgumentList 'serve' -NoNewWindow -RedirectStandardOutput ollama.log -RedirectStandardError ollama_error.log

              Write-Host 'Ollama server process started'
            } catch {
              Write-Host 'Error starting Ollama: $_'
              exit 1
            }
          }"
        else
          # Unix-like systems (Linux/macOS)
          ollama serve > ollama.log 2>&1 &
          echo $! > ollama.pid
        fi

        # Server readiness check
        echo "Waiting for server to be ready..."
        TIMEOUT=30
        READY=0
        while [ $TIMEOUT -gt 0 ]; do
          if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
            powershell -Command "
              if (Test-NetConnection -ComputerName localhost -Port 11434 -WarningAction SilentlyContinue) {
                exit 0
              } else {
                exit 1
              }
            " && READY=1 && break
          else
            curl -s http://localhost:11434/api/version >/dev/null && READY=1 && break
          fi
          echo "Waiting for server... ($TIMEOUT seconds remaining)"
          sleep 1
          TIMEOUT=$((TIMEOUT - 1))
        done

        if [ $READY -eq 0 ]; then
          echo "Failed to start Ollama server"
          cat ollama.log
          [ -f ollama_error.log ] && cat ollama_error.log
          exit 1
        fi

        echo "Ollama server is running"

    # Pull Model
    - name: Pull Model
      shell: bash
      run: |
        if [ "${{ steps.platform.outputs.platform }}" = "macos" ]; then
          gtimeout ${{ inputs.timeout }} ollama pull ${{ inputs.model }}
        else
          timeout ${{ inputs.timeout }} ollama pull ${{ inputs.model }}
        fi

    - name: Execute Command
      shell: bash
      run: |
        if [ "${{ inputs.command }}" = "serve" ]; then
          echo "Ollama is running in server mode"
          sleep ${{ inputs.timeout }}
        else
          echo "Running model with prompt..."
          if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
            OLLAMA_EXE="C:/ollama/ollama.exe"
            powershell -Command "& '$OLLAMA_EXE' run ${{ inputs.model }} '${{ inputs.prompt }}'"
          else
            ollama run ${{ inputs.model }} "${{ inputs.prompt }}"
          fi
        fi

    # Cleanup
    - name: Cleanup
      if: always()
      shell: bash
      run: |
        echo "Cleaning up Ollama processes..."
        if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
          powershell -Command "Stop-Process -Name ollama -Force -ErrorAction SilentlyContinue"
        else
          if [ -f ollama.pid ]; then
            kill $(cat ollama.pid) || true
          else
            pkill ollama || true
          fi
        fi

        # Display logs for debugging
        echo "Server logs:"
        cat ollama.log
        [ -f ollama_error.log ] && echo "Error logs:" && cat ollama_error.log
