name: "Ollama Runner Action"
description: "Install and run Ollama with specified models"
author: "Your Name"

inputs:
  model:
    description: "Ollama model to use (e.g., llama2, codellama, mistral)"
    required: true
    default: "llama2"
  command:
    description: "Ollama command to run (run/serve)"
    required: false
    default: "run"
  prompt:
    description: "Prompt to send to the model"
    required: false
    default: "Hello, how are you?"
  timeout:
    description: "Timeout in seconds for operations"
    required: false
    default: "300"

runs:
  using: "composite"
  steps:
    # Platform Detection
    - name: Detect Platform
      id: platform
      shell: bash
      run: |
        case "$(uname -s)" in
          Linux*)     echo "platform=linux" >> $GITHUB_OUTPUT;;
          Darwin*)    echo "platform=macos" >> $GITHUB_OUTPUT;;
          MINGW64*)   echo "platform=windows" >> $GITHUB_OUTPUT;;
          *)         echo "Unknown platform" && exit 1;;
        esac

    # Linux Installation
    - name: Install Ollama (Linux)
      if: steps.platform.outputs.platform == 'linux'
      shell: bash
      run: |
        curl -fsSL https://ollama.ai/install.sh | sh
        echo "Installed Ollama for Linux"

    - name: Install Ollama (macOS)
      if: steps.platform.outputs.platform == 'macos'
      shell: bash
      run: |
        # Check if Homebrew is installed, install if not
        if ! command -v brew &> /dev/null; then
          echo "Installing Homebrew..."
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
        fi

        # Install coreutils and Ollama using Homebrew
        echo "Installing dependencies and Ollama using Homebrew..."
        brew install coreutils ollama

        echo "Ollama installed successfully"

    - name: Install Ollama (Windows)
      if: steps.platform.outputs.platform == 'windows'
      shell: pwsh
      run: |
        Write-Host "Installing Ollama for Windows..."

        try {
          # Get latest release info
          $apiLatestUrl = "https://api.github.com/repos/ollama/ollama/releases/latest"
          $latestRelease = Invoke-RestMethod -Uri $apiLatestUrl
          $zipUrl = $latestRelease.assets | Where-Object { $_.name -match "windows-amd64\.zip$" } | Select-Object -ExpandProperty browser_download_url

          # Create installation directory
          $installPath = "${{ inputs.install_path }}"
          if (-not (Test-Path $installPath)) {
            New-Item -Path $installPath -ItemType Directory -Force
          }

          # Download and extract
          Write-Host "Downloading Ollama..."
          Invoke-WebRequest -Uri $zipUrl -OutFile "ollama.zip"

          Write-Host "Extracting Ollama..."
          Expand-Archive -Path "ollama.zip" -DestinationPath $installPath -Force

          # Add to PATH
          $env:PATH = "$installPath;$env:PATH"
          Write-Host "PATH=$env:PATH" | Out-File -FilePath $env:GITHUB_ENV -Append

          Write-Host "Ollama installed successfully"

        } catch {
          Write-Host "Error during installation: $_"
          Write-Host $_.ScriptStackTrace
          exit 1
        } finally {
          # Cleanup
          if (Test-Path "ollama.zip") {
            Remove-Item "ollama.zip" -Force
          }
        }
    # Start Ollama Server (Platform Independent)
    - name: Start Ollama Server
      shell: bash
      run: |
        echo "Starting Ollama server..."

        if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
          # Windows-specific start
          powershell -Command "Start-Process ollama -ArgumentList 'serve' -NoNewWindow -RedirectStandardOutput ollama.log -RedirectStandardError ollama_error.log"
        else
          # Unix-like systems (Linux/macOS)
          ollama serve > ollama.log 2>&1 &
          echo $! > ollama.pid
        fi

        # More robust server check
        echo "Waiting for server to be ready..."
        TIMEOUT=30
        READY=0
        while [ $TIMEOUT -gt 0 ]; do
          if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
            # Windows check
            powershell -Command "Test-NetConnection -ComputerName localhost -Port 11434 -WarningAction SilentlyContinue" && READY=1 && break
          else
            # Unix check
            curl -s http://localhost:11434/api/version >/dev/null && READY=1 && break
          fi
          echo "Waiting for server... ($TIMEOUT seconds remaining)"
          sleep 1
          TIMEOUT=$((TIMEOUT - 1))
        done

        if [ $READY -eq 0 ]; then
          echo "Failed to start Ollama server"
          cat ollama.log
          [ -f ollama_error.log ] && cat ollama_error.log
          exit 1
        fi

        echo "Ollama server is running"

    # Pull Model
    - name: Pull Model
      shell: bash
      run: |
        if [ "${{ steps.platform.outputs.platform }}" = "macos" ]; then
          gtimeout ${{ inputs.timeout }} ollama pull ${{ inputs.model }}
        else
          timeout ${{ inputs.timeout }} ollama pull ${{ inputs.model }}
        fi

    # Execute Command
    - name: Execute Command
      shell: bash
      run: |
        if [ "${{ inputs.command }}" = "serve" ]; then
          echo "Ollama is running in server mode"
          sleep ${{ inputs.timeout }}
        else
          echo "Running model with prompt..."
          RESPONSE=$(ollama run ${{ inputs.model }} "${{ inputs.prompt }}" 2>&1)
          EXIT_CODE=$?
          echo "$RESPONSE"

          if [ $EXIT_CODE -ne 0 ]; then
            echo "Error running model command"
            exit $EXIT_CODE
          fi
        fi

    # Cleanup
    - name: Cleanup
      if: always()
      shell: bash
      run: |
        echo "Cleaning up Ollama processes..."
        if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
          powershell -Command "Stop-Process -Name ollama -Force -ErrorAction SilentlyContinue"
        else
          if [ -f ollama.pid ]; then
            kill $(cat ollama.pid) || true
          else
            pkill ollama || true
          fi
        fi

        # Display logs for debugging
        echo "Server logs:"
        cat ollama.log
        [ -f ollama_error.log ] && echo "Error logs:" && cat ollama_error.log
