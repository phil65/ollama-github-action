name: "Ollama Runner Action"
description: "Install and run Ollama with specified models"
author: "Your Name"

inputs:
  model:
    description: "Ollama model to use (e.g., llama2, codellama, mistral)"
    required: true
    default: "llama2"
  command:
    description: "Ollama command to run (run/serve)"
    required: false
    default: "run"
  prompt:
    description: "Prompt to send to the model"
    required: false
    default: "Hello, how are you?"
  timeout:
    description: "Timeout in seconds for operations"
    required: false
    default: "300"

runs:
  using: "composite"
  steps:
    # Platform Detection
    - name: Detect Platform
      id: platform
      shell: bash
      run: |
        case "$(uname -s)" in
          Linux*)     echo "platform=linux" >> $GITHUB_OUTPUT;;
          Darwin*)    echo "platform=macos" >> $GITHUB_OUTPUT;;
          MINGW64*)   echo "platform=windows" >> $GITHUB_OUTPUT;;
          *)         echo "Unknown platform" && exit 1;;
        esac

    # Linux Installation
    - name: Install Ollama (Linux)
      if: steps.platform.outputs.platform == 'linux'
      shell: bash
      run: |
        curl -fsSL https://ollama.ai/install.sh | sh
        echo "Installed Ollama for Linux"

    - name: Install Ollama (macOS)
      if: steps.platform.outputs.platform == 'macos'
      shell: bash
      run: |
        # Check if Homebrew is installed, install if not
        if ! command -v brew &> /dev/null; then
          echo "Installing Homebrew..."
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
        fi

        # Install coreutils and Ollama using Homebrew
        echo "Installing dependencies and Ollama using Homebrew..."
        brew install coreutils ollama

        echo "Ollama installed successfully"

    - name: Install Ollama (Windows)
      if: steps.platform.outputs.platform == 'windows'
      shell: pwsh
      run: |
        Write-Host "Setting up winget..."

        try {
          # Install winget
          $apiLatestUrl = "https://api.github.com/repos/microsoft/winget-cli/releases/latest"
          $latestRelease = Invoke-RestMethod -Uri $apiLatestUrl
          $msixBundleUrl = $latestRelease.assets | Where-Object { $_.name -match "\.msixbundle$" } | Select-Object -ExpandProperty browser_download_url

          Write-Host "Downloading winget..."
          Invoke-WebRequest -Uri $msixBundleUrl -OutFile "winget.msixbundle"

          Write-Host "Installing winget..."
          Add-AppxPackage -Path "winget.msixbundle"

          # Install Ollama using winget
          Write-Host "Installing Ollama using winget..."
          winget install Ollama.Ollama --accept-source-agreements --accept-package-agreements

          # Update PATH environment variable
          $env:PATH = [System.Environment]::GetEnvironmentVariable("Path","Machine") + ";" + [System.Environment]::GetEnvironmentVariable("Path","User")
          Write-Host "PATH=$env:PATH" | Out-File -FilePath $env:GITHUB_ENV -Append

          Write-Host "Ollama installed successfully"

        } catch {
          Write-Host "Error during installation: $_"
          Write-Host $_.ScriptStackTrace
          exit 1
        } finally {
          # Cleanup
          if (Test-Path "winget.msixbundle") {
            Remove-Item "winget.msixbundle" -Force
          }
        }

    # Start Ollama Server (Platform Independent)
    - name: Start Ollama Server
      shell: bash
      run: |
        # Start Ollama in background
        if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
          start /B ollama serve > ollama.log 2>&1
        else
          ollama serve > ollama.log 2>&1 &
        fi
        echo $! > ollama.pid

        # Wait for server
        TIMEOUT=30
        while [ $TIMEOUT -gt 0 ]; do
          if nc -z localhost 11434 2>/dev/null; then
            echo "Ollama server is running"
            break
          fi
          sleep 1
          TIMEOUT=$((TIMEOUT - 1))
        done

        if [ $TIMEOUT -eq 0 ]; then
          echo "Failed to start Ollama server"
          exit 1
        fi

    # Pull Model
    - name: Pull Model
      shell: bash
      run: |
        if [ "${{ steps.platform.outputs.platform }}" = "macos" ]; then
          gtimeout ${{ inputs.timeout }} ollama pull ${{ inputs.model }}
        else
          timeout ${{ inputs.timeout }} ollama pull ${{ inputs.model }}
        fi

    # Execute Command
    - name: Execute Command
      shell: bash
      run: |
        if [ "${{ inputs.command }}" = "serve" ]; then
          echo "Ollama is running in server mode"
          sleep ${{ inputs.timeout }}
        else
          if [ "${{ steps.platform.outputs.platform }}" = "macos" ]; then
            gtimeout ${{ inputs.timeout }} ollama run ${{ inputs.model }} "${{ inputs.prompt }}"
          else
            timeout ${{ inputs.timeout }} ollama run ${{ inputs.model }} "${{ inputs.prompt }}"
          fi
        fi

    # Cleanup
    - name: Cleanup
      if: always()
      shell: bash
      run: |
        if [ "${{ steps.platform.outputs.platform }}" = "windows" ]; then
          taskkill /F /IM ollama.exe || true
        else
          if [ -f ollama.pid ]; then
            kill $(cat ollama.pid) || true
          fi
        fi
